{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0144924",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers accelerate hf_xet -q\n",
    "\n",
    "print(\"‚úì Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project path to Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = \"/content/drive/MyDrive/DATA 298A/sjsu-data298-main\"\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "print(f\"‚úì Project path: {project_path}\")\n",
    "print(f\"‚úì Path exists: {os.path.exists(project_path)}\")\n",
    "\n",
    "if os.path.exists(project_path):\n",
    "    contents = os.listdir(project_path)\n",
    "    print(f\"‚úì Contents: {contents}\")\n",
    "    \n",
    "    # Check for the wrapper file\n",
    "    if \"medical_llm_wrapper.py\" in contents:\n",
    "        print(\"‚úì medical_llm_wrapper.py found!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: medical_llm_wrapper.py NOT FOUND!\")\n",
    "        print(\"   Please upload medical_llm_wrapper.py to:\")\n",
    "        print(f\"   {project_path}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  ERROR: Project path does not exist!\")\n",
    "    print(f\"   Please create the folder: {project_path}\")\n",
    "    print(f\"   Or update the path above to match your Google Drive structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wrapper\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Double-check the path is added (in case cells ran out of order)\n",
    "project_path = \"/content/drive/MyDrive/DATA 298A/sjsu-data298-main\"\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "    print(f\"‚úì Added {project_path} to Python path\")\n",
    "\n",
    "# Verify the file exists\n",
    "wrapper_file = os.path.join(project_path, \"medical_llm_wrapper.py\")\n",
    "if os.path.exists(wrapper_file):\n",
    "    print(f\"‚úì Found: {wrapper_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"medical_llm_wrapper.py not found at {wrapper_file}\")\n",
    "\n",
    "# Now import\n",
    "try:\n",
    "    from medical_llm_wrapper import MedicalLLMWrapper, load_medical_llm\n",
    "    import torch\n",
    "    print(\"‚úì Medical LLM Wrapper imported successfully!\")\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nüìã Troubleshooting steps:\")\n",
    "    print(\"1. RESTART THE RUNTIME: Runtime ‚Üí Restart runtime\")\n",
    "    print(\"2. Re-run all cells from the beginning\")\n",
    "    print(\"3. Make sure you run the 'Add project path' cell BEFORE this cell\")\n",
    "    print(f\"\\nüí° Current sys.path:\")\n",
    "    for p in sys.path[:5]:\n",
    "        print(f\"   {p}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54e017",
   "metadata": {},
   "source": [
    "## 2. Test 1: MedGemma-4B-IT - Multiple Choice Question\n",
    "\n",
    "Test the wrapper with MedGemma on a clinical diagnosis MCQ.\n",
    "- **Automatic fp32 conversion** (MedGemma requires float32)\n",
    "- **Constrained generation** (forces A/B/C/D)\n",
    "- **Answer + Rationale mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: MedGemma-4B-IT - MCQ with Answer + Rationale\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load MedGemma (auto-converts to fp32 when torch_dtype=None)\n",
    "medgemma = load_medical_llm(\n",
    "    \"google/medgemma-4b-it\",\n",
    "    device=\"cuda\"\n",
    "    # Don't pass torch_dtype - let wrapper auto-detect and use fp32 for MedGemma\n",
    ")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\n[Model Information]\")\n",
    "info = medgemma.get_model_info()\n",
    "for key, value in info.items():\n",
    "    if key != \"num_parameters\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "\n",
    "# Set task type\n",
    "medgemma.set_task(\"mcq\")\n",
    "medgemma.set_mode(\"answer_rationale\")\n",
    "\n",
    "# Medical MCQ prompt\n",
    "mcq_prompt = \"\"\"A 55-year-old patient presents with persistent cough, hemoptysis, and unintentional weight loss.\n",
    "Chest X-ray shows a mass in the right upper lobe. What is the most likely diagnosis?\n",
    "\n",
    "A) Tuberculosis\n",
    "B) Lung cancer\n",
    "C) Pneumonia\n",
    "D) Pulmonary embolism\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[Generating Response...]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = medgemma.generate(mcq_prompt)\n",
    "\n",
    "print(\"\\n[RESULT]\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 1 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6e13c",
   "metadata": {},
   "source": [
    "## 3. Test 2: MedGemma - Answer Only Mode with Confidence\n",
    "\n",
    "Test confidence extraction for MCQ answers.\n",
    "- **Answer-only mode** (no rationale)\n",
    "- **Confidence scores** for each option\n",
    "- **Probability distribution** over A/B/C/D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ec148",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: MedGemma - MCQ with Confidence Scores\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Switch to answer-only mode\n",
    "medgemma.set_mode(\"answer_only\")\n",
    "\n",
    "# Generate with confidence\n",
    "response = medgemma.generate(mcq_prompt)\n",
    "\n",
    "print(\"\\n[RESULT]\")\n",
    "print(response)\n",
    "print(f\"\\n[CONFIDENCE METRICS]\")\n",
    "print(f\"  Selected Answer: {medgemma.last_answer}\")\n",
    "\n",
    "# Handle NaN confidence values\n",
    "import math\n",
    "if medgemma.last_confidence is not None and not math.isnan(medgemma.last_confidence):\n",
    "    print(f\"  Confidence: {medgemma.last_confidence:.4f}\")\n",
    "    print(f\"\\n  Option Probabilities:\")\n",
    "    for option, prob in sorted(medgemma.last_option_probs.items()):\n",
    "        if not math.isnan(prob):\n",
    "            bar = \"‚ñà\" * int(prob * 50)\n",
    "            print(f\"    {option}: {prob:.4f} {bar}\")\n",
    "        else:\n",
    "            print(f\"    {option}: NaN (computation error)\")\n",
    "else:\n",
    "    print(f\"  Confidence: NaN (computation error)\")\n",
    "    print(f\"\\n  Note: Confidence computation failed. This may happen with fp32 models.\")\n",
    "    print(f\"  The answer '{medgemma.last_answer}' was still generated successfully.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 2 Complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up memory\n",
    "del medgemma\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d00b5",
   "metadata": {},
   "source": [
    "## 4. Test 3: Apollo-2B - Yes/No Question\n",
    "\n",
    "Test the wrapper with a different model (Apollo) on a binary Yes/No question.\n",
    "- **Different architecture** (Llama-based vs Gemma-based)\n",
    "- **Native fp16** (no conversion needed)\n",
    "- **Yes/No task** (A=Yes, B=No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c51f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 3: Apollo-2B - Yes/No Question with Answer + Rationale\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load Apollo (works fine in fp16)\n",
    "apollo = load_medical_llm(\n",
    "    \"FreedomIntelligence/Apollo-2B\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\n[Model Information]\")\n",
    "info = apollo.get_model_info()\n",
    "for key, value in info.items():\n",
    "    if key != \"num_parameters\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "\n",
    "# Set task type to Yes/No\n",
    "apollo.set_task(\"yn\")\n",
    "apollo.set_mode(\"answer_rationale\")\n",
    "\n",
    "# Yes/No medical question\n",
    "yn_prompt = \"\"\"Metformin is contraindicated in patients with severe renal impairment.\n",
    "\n",
    "A) Yes\n",
    "B) No\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[Generating Response...]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = apollo.generate(yn_prompt)\n",
    "\n",
    "print(\"\\n[RESULT]\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 3 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bc10d",
   "metadata": {},
   "source": [
    "## 5. Test 4: Apollo - Free-Response Generation\n",
    "\n",
    "Test unconstrained generation for open-ended medical questions.\n",
    "- **Free-response task** (no answer constraints)\n",
    "- **Longer generation** (up to 200 tokens)\n",
    "- **Medical knowledge test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce817b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 4: Apollo-2B - Free-Response Medical Question\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set to free-response task\n",
    "apollo.set_task(\"free\")\n",
    "\n",
    "# Open-ended medical question\n",
    "free_prompt = \"What are the first-line treatments for hypertension in a 60-year-old patient?\"\n",
    "\n",
    "print(\"\\n[Generating Response...]\")\n",
    "\n",
    "response = apollo.generate(free_prompt)\n",
    "\n",
    "print(\"\\n[RESULT]\")\n",
    "print(f\"Question: {free_prompt}\")\n",
    "print(f\"Answer: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 4 Complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up memory\n",
    "del apollo\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0848282",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 6: BioMistral-7B - Yes/No Question with Rationale\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load BioMistral-7B\n",
    "biomistral = load_medical_llm(\n",
    "    \"BioMistral/BioMistral-7B\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\n[Model Information]\")\n",
    "info = biomistral.get_model_info()\n",
    "for key, value in info.items():\n",
    "    if key != \"num_parameters\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "\n",
    "# Set task type\n",
    "biomistral.set_task(\"yn\")\n",
    "biomistral.set_mode(\"answer_rationale\")\n",
    "\n",
    "# Yes/No medical question\n",
    "biomistral_prompt = \"\"\"Corticosteroids are first-line treatment for acute bacterial meningitis.\n",
    "\n",
    "A) True\n",
    "B) False\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[Generating Response...]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = biomistral.generate(biomistral_prompt)\n",
    "\n",
    "print(\"\\n[RESULT]\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 6 Complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up memory\n",
    "del biomistral\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd6618",
   "metadata": {},
   "source": [
    "## 6. Test 6: BioMistral-7B - Yes/No Medical Question\n",
    "\n",
    "Test the wrapper with BioMistral-7B model.\n",
    "- **7B parameter model** (largest model in this demo)\n",
    "- **Mistral architecture** fine-tuned on biomedical data\n",
    "- **Yes/No task** with answer + rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 5: BioMedLM - Medical MCQ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load BioMedLM\n",
    "biomedlm = load_medical_llm(\n",
    "    \"stanford-crfm/BioMedLM\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Display model info\n",
    "print(\"\\n[Model Information]\")\n",
    "info = biomedlm.get_model_info()\n",
    "for key, value in info.items():\n",
    "    if key != \"num_parameters\":\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "\n",
    "# Set task type\n",
    "biomedlm.set_task(\"mcq\")\n",
    "biomedlm.set_mode(\"answer_only\")\n",
    "\n",
    "# Medical MCQ prompt\n",
    "biomedlm_prompt = \"\"\"Which class of antibiotics inhibits bacterial cell wall synthesis?\n",
    "A) Fluoroquinolones\n",
    "B) Tetracyclines\n",
    "C) Beta-lactams\n",
    "D) Aminoglycosides\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[Generating Response...]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = biomedlm.generate(biomedlm_prompt)\n",
    "\n",
    "print(\"\\n[RESULT]\")\n",
    "print(response)\n",
    "print(f\"\\n[CONFIDENCE METRICS]\")\n",
    "print(f\"  Selected Answer: {biomedlm.last_answer}\")\n",
    "\n",
    "# Handle NaN confidence values\n",
    "import math\n",
    "if biomedlm.last_confidence is not None and not math.isnan(biomedlm.last_confidence):\n",
    "    print(f\"  Confidence: {biomedlm.last_confidence:.4f}\")\n",
    "    print(f\"\\n  Option Probabilities:\")\n",
    "    for option, prob in sorted(biomedlm.last_option_probs.items()):\n",
    "        if not math.isnan(prob):\n",
    "            bar = \"‚ñà\" * int(prob * 50)\n",
    "            print(f\"    {option}: {prob:.4f} {bar}\")\n",
    "        else:\n",
    "            print(f\"    {option}: NaN (computation error)\")\n",
    "else:\n",
    "    print(f\"  Confidence: NaN (computation error)\")\n",
    "    print(f\"\\n  Note: Confidence computation failed for this model.\")\n",
    "    print(f\"  The answer '{biomedlm.last_answer}' was still generated successfully.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 5 Complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up memory\n",
    "del biomedlm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bd28ef",
   "metadata": {},
   "source": [
    "## 5. Test 5: BioMedLM - Medical MCQ\n",
    "\n",
    "Test the wrapper with Stanford's BioMedLM model.\n",
    "- **2.7B parameter model** trained on biomedical literature\n",
    "- **GPT-2 architecture** (different from Gemma/Llama)\n",
    "- **MCQ task** with confidence scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387a94f",
   "metadata": {},
   "source": [
    "## 6. Test 5: Batch Processing - Multiple Prompts\n",
    "\n",
    "Test batch generation functionality with multiple prompts.\n",
    "- **Batch processing** of multiple questions\n",
    "- **Progress tracking**\n",
    "- **Efficient memory management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 5: MedGemma - Batch Processing Multiple MCQs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reload MedGemma for batch testing\n",
    "medgemma = load_medical_llm(\n",
    "    \"google/medgemma-4b-it\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "medgemma.set_task(\"mcq\")\n",
    "medgemma.set_mode(\"answer_only\")\n",
    "\n",
    "# Multiple MCQ prompts\n",
    "batch_prompts = [\n",
    "    \"\"\"Which vitamin deficiency causes scurvy?\n",
    "A) Vitamin A\n",
    "B) Vitamin B12\n",
    "C) Vitamin C\n",
    "D) Vitamin D\n",
    "\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"\"\"What is the normal range for fasting blood glucose?\n",
    "A) 50-70 mg/dL\n",
    "B) 70-100 mg/dL\n",
    "C) 100-125 mg/dL\n",
    "D) 125-150 mg/dL\n",
    "\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"\"\"Which organ produces insulin?\n",
    "A) Liver\n",
    "B) Pancreas\n",
    "C) Kidney\n",
    "D) Spleen\n",
    "\n",
    "Answer:\"\"\"\n",
    "]\n",
    "\n",
    "print(\"\\n[Batch Generation Starting...]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = medgemma.batch_generate(batch_prompts, show_progress=True)\n",
    "\n",
    "print(\"\\n[RESULTS]\")\n",
    "for i, (prompt, result) in enumerate(zip(batch_prompts, results), 1):\n",
    "    question = prompt.split('\\n')[0]\n",
    "    print(f\"\\n{i}. {question}\")\n",
    "    print(f\"   {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 5 Complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up memory\n",
    "del medgemma\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0a939",
   "metadata": {},
   "source": [
    "## 7. Test 6: Cross-Model Comparison - Same Question\n",
    "\n",
    "Compare how different models answer the same medical question.\n",
    "- **MedGemma vs Apollo** on identical prompt\n",
    "- **Confidence comparison**\n",
    "- **Model-agnostic API** demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 6: Cross-Model Comparison - MedGemma vs Apollo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Comparison prompt\n",
    "comparison_prompt = \"\"\"Aspirin is used for primary prevention of cardiovascular disease in high-risk patients.\n",
    "\n",
    "A) True\n",
    "B) False\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(f\"\\nQuestion: {comparison_prompt.split('Answer:')[0].strip()}\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "models_to_test = [\n",
    "    (\"google/medgemma-4b-it\", \"MedGemma-4B-IT\"),\n",
    "    (\"FreedomIntelligence/Apollo-2B\", \"Apollo-2B\")\n",
    "]\n",
    "\n",
    "results_comparison = {}\n",
    "\n",
    "for model_name, display_name in models_to_test:\n",
    "    print(f\"\\n[Loading {display_name}...]\")\n",
    "    \n",
    "    wrapper = load_medical_llm(model_name, device=\"cuda\")\n",
    "    wrapper.set_task(\"yn\")\n",
    "    wrapper.set_mode(\"answer_only\")\n",
    "    \n",
    "    response = wrapper.generate(comparison_prompt)\n",
    "    \n",
    "    results_comparison[display_name] = {\n",
    "        \"answer\": wrapper.last_answer,\n",
    "        \"confidence\": wrapper.last_confidence,\n",
    "        \"option_probs\": wrapper.last_option_probs\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì {display_name} complete\")\n",
    "    \n",
    "    # Clean up\n",
    "    del wrapper\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Display comparison results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[COMPARISON RESULTS]\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, results in results_comparison.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Answer: {results['answer']}\")\n",
    "    \n",
    "    # Handle NaN confidence\n",
    "    import math\n",
    "    conf = results['confidence']\n",
    "    if conf is not None and not math.isnan(conf):\n",
    "        print(f\"  Confidence: {conf:.4f}\")\n",
    "        print(f\"  Probabilities:\")\n",
    "        for option, prob in sorted(results['option_probs'].items()):\n",
    "            if not math.isnan(prob):\n",
    "                bar = \"‚ñà\" * int(prob * 40)\n",
    "                print(f\"    {option}: {prob:.4f} {bar}\")\n",
    "            else:\n",
    "                print(f\"    {option}: NaN\")\n",
    "    else:\n",
    "        print(f\"  Confidence: NaN (computation error)\")\n",
    "        print(f\"  Note: Answer generated successfully, but confidence calculation failed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úì Test 6 Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6bdeb",
   "metadata": {},
   "source": [
    "## 8. Summary of Results\n",
    "\n",
    "### ‚úÖ Successfully Tested:\n",
    "\n",
    "1. **MedGemma-4B-IT**\n",
    "   - MCQ with answer + rationale\n",
    "   - Confidence scoring\n",
    "   - Automatic fp32 conversion\n",
    "   - Batch processing\n",
    "\n",
    "2. **Apollo-2B**\n",
    "   - Yes/No questions\n",
    "   - Free-response generation\n",
    "   - Native fp16 operation\n",
    "   - Cross-model comparison\n",
    "\n",
    "3. **Wrapper Features**\n",
    "   - Model-agnostic loading\n",
    "   - Task type switching (yn/mcq/free)\n",
    "   - Mode switching (answer_rationale/answer_only)\n",
    "   - Batch generation with progress\n",
    "   - Confidence extraction\n",
    "   - Model metadata access\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "- **Same API works for all models** - true model-agnostic design ‚úÖ\n",
    "- **Automatic dtype handling** - MedGemma ‚Üí fp32, others ‚Üí native dtype ‚úÖ\n",
    "- **Robust tokenizer handling** - works across different tokenizer implementations ‚úÖ\n",
    "- **Memory efficient** - proper cleanup between model loads ‚úÖ\n",
    "\n",
    "### Potential Use Cases:\n",
    "\n",
    "1. **Medical Question Answering**: Structured MCQ evaluation\n",
    "2. **Clinical Decision Support**: Confidence-weighted recommendations\n",
    "3. **Model Benchmarking**: Compare multiple models on same tasks\n",
    "4. **Educational Tools**: Generate explanations with rationales\n",
    "5. **Research**: Model interpretability and comparison studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d96ebd",
   "metadata": {},
   "source": [
    "## 9. Optional: Additional Testing\n",
    "\n",
    "You can add more models or custom tests here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d303759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test with your own prompt\n",
    "def custom_test(model_name, prompt, task_type=\"mcq\", mode=\"answer_rationale\"):\n",
    "    \"\"\"\n",
    "    Run a custom test with any model and prompt.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"CUSTOM TEST: {model_name}\")\n",
    "    print('=' * 80)\n",
    "    \n",
    "    wrapper = load_medical_llm(model_name, device=\"cuda\")\n",
    "    wrapper.set_task(task_type)\n",
    "    wrapper.set_mode(mode)\n",
    "    \n",
    "    print(f\"\\nPrompt:\\n{prompt}\")\n",
    "    print(\"\\n[Generating...]\")\n",
    "    \n",
    "    response = wrapper.generate(prompt)\n",
    "    \n",
    "    print(f\"\\n[Result]\")\n",
    "    print(response)\n",
    "    \n",
    "    if mode == \"answer_only\":\n",
    "        print(f\"\\nConfidence: {wrapper.last_confidence:.4f}\")\n",
    "        if wrapper.last_option_probs:\n",
    "            print(\"Option Probabilities:\")\n",
    "            for opt, prob in sorted(wrapper.last_option_probs.items()):\n",
    "                print(f\"  {opt}: {prob:.4f}\")\n",
    "    \n",
    "    del wrapper\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print('=' * 80)\n",
    "\n",
    "# Uncomment to run custom test:\n",
    "# custom_test(\n",
    "#     \"google/medgemma-4b-it\",\n",
    "#     \"Your medical question here...\\nA) Option A\\nB) Option B\\nAnswer:\",\n",
    "#     task_type=\"mcq\",\n",
    "#     mode=\"answer_rationale\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de91b82",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Final Notes\n",
    "\n",
    "### Memory Management:\n",
    "- Each model is properly unloaded after testing\n",
    "- `torch.cuda.empty_cache()` clears GPU memory\n",
    "- Batch operations handle memory efficiently\n",
    "\n",
    "### Model Requirements:\n",
    "- **MedGemma**: Gated model, requires HF token, needs ~12GB GPU (fp32)\n",
    "- **Apollo**: Public model, no token needed, needs ~4GB GPU (fp16)\n",
    "\n",
    "### Next Steps:\n",
    "- Test with additional models (BioMistral, BioMedLM)\n",
    "- Implement custom LogitsProcessors for domain-specific constraints\n",
    "- Add few-shot prompting examples\n",
    "- Export results for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd42e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"‚úì All tests complete!\")\n",
    "print(\"‚úì Memory cleaned up\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Medical LLM Wrapper Demo - SUCCESS!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
