{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8dece1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5cf635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers accelerate tqdm -q\n",
    "\n",
    "print(\"✓ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f95bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = \"/content/drive/MyDrive/DATA 298A/sjsu-data298-main\"\n",
    "\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "\n",
    "print(f\"✓ Project path: {project_path}\")\n",
    "print(f\"✓ Path exists: {os.path.exists(project_path)}\")\n",
    "\n",
    "if os.path.exists(project_path):\n",
    "    contents = os.listdir(project_path)\n",
    "    print(f\"✓ Contents: {contents}\")\n",
    "    \n",
    "    required_files = ['medical_llm_wrapper.py', 'medical_integrated_gradients.py']\n",
    "    for file in required_files:\n",
    "        if file in contents:\n",
    "            print(f\"✓ {file} found!\")\n",
    "        else:\n",
    "            print(f\"⚠️  WARNING: {file} NOT FOUND!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44022865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "\n",
    "from medical_llm_wrapper import load_medical_llm\n",
    "from medical_integrated_gradients import (\n",
    "    MedicalIntegratedGradients,\n",
    "    explain_medical_prediction,\n",
    "    visualize_attributions\n",
    ")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fec503",
   "metadata": {},
   "source": [
    "## 2. Example 1: Basic Usage - MedGemma MCQ\n",
    "\n",
    "Let's explain why MedGemma chose a particular answer for a clinical diagnosis question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 1: MedGemma - Clinical Diagnosis MCQ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load MedGemma\n",
    "medgemma = load_medical_llm(\n",
    "    \"google/medgemma-4b-it\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# Set task type\n",
    "medgemma.set_task(\"mcq\")\n",
    "medgemma.set_mode(\"answer_only\")\n",
    "\n",
    "print(\"\\n✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5df927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define medical MCQ\n",
    "prompt = \"\"\"A 65-year-old man presents with sudden onset chest pain, dyspnea, and diaphoresis. \n",
    "ECG shows ST-segment elevation in leads V1-V4. What is the most likely diagnosis?\n",
    "\n",
    "A) Unstable angina\n",
    "B) Anterior myocardial infarction\n",
    "C) Pulmonary embolism\n",
    "D) Aortic dissection\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Get model's prediction first\n",
    "print(\"\\n[Getting Model Prediction...]\")\n",
    "response = medgemma.generate(prompt)\n",
    "print(f\"\\nModel Answer: {medgemma.last_answer}\")\n",
    "print(f\"Confidence: {medgemma.last_confidence:.4f}\" if not np.isnan(medgemma.last_confidence) else \"Confidence: NaN\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain prediction with Integrated Gradients\n",
    "print(\"\\n[Computing Integrated Gradients...]\")\n",
    "print(\"This will take ~30 seconds...\\n\")\n",
    "\n",
    "result = explain_medical_prediction(\n",
    "    wrapper=medgemma,\n",
    "    prompt=prompt,\n",
    "    target_class=\"B\",  # Explain why model chose \"B\" (or choose model's actual answer)\n",
    "    n_steps=50,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "print(f\"\\n[Key Insights]\")\n",
    "print(f\"  Target Probability: {result['target_probability']:.4f}\")\n",
    "print(f\"  Convergence Delta: {result['convergence_delta']:.6f} (lower is better)\")\n",
    "print(f\"\\n  Top 5 Most Important Tokens:\")\n",
    "\n",
    "# Get top tokens\n",
    "token_scores = list(zip(result['tokens'], result['attributions']))\n",
    "token_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "for i, (token, score) in enumerate(token_scores[:5], 1):\n",
    "    clean_token = token.replace('▁', ' ').replace('Ġ', ' ').strip()\n",
    "    print(f\"    {i}. '{clean_token}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0e8d5",
   "metadata": {},
   "source": [
    "## 3. Example 2: Yes/No Question - Apollo\n",
    "\n",
    "Let's see how attributions work for binary medical questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147335bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 2: Apollo - Yes/No Medical Question\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load Apollo\n",
    "apollo = load_medical_llm(\n",
    "    \"FreedomIntelligence/Apollo-2B\",\n",
    "    device=\"cuda\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "apollo.set_task(\"yn\")\n",
    "apollo.set_mode(\"answer_only\")\n",
    "\n",
    "print(\"\\n✓ Apollo loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Yes/No question\n",
    "yn_prompt = \"\"\"Metformin is contraindicated in patients with severe renal impairment \n",
    "(eGFR < 30 mL/min/1.73m²) due to increased risk of lactic acidosis.\n",
    "\n",
    "A) True\n",
    "B) False\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Get prediction\n",
    "print(\"\\n[Getting Prediction...]\")\n",
    "response = apollo.generate(yn_prompt)\n",
    "print(f\"\\nModel Answer: {apollo.last_answer} ({'True' if apollo.last_answer == 'A' else 'False'})\")\n",
    "print(f\"Confidence: {apollo.last_confidence:.4f}\" if not np.isnan(apollo.last_confidence) else \"Confidence: NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344da9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain with IG\n",
    "print(\"\\n[Computing Integrated Gradients...]\\n\")\n",
    "\n",
    "result = explain_medical_prediction(\n",
    "    wrapper=apollo,\n",
    "    prompt=yn_prompt,\n",
    "    target_class=\"A\",  # Explain \"True\" answer\n",
    "    n_steps=50,\n",
    "    visualize=True\n",
    ")\n",
    "\n",
    "print(f\"\\n[Analysis]\")\n",
    "print(f\"  The model assigned {result['target_probability']:.1%} probability to 'True'\")\n",
    "print(f\"  Most influential tokens support {'True' if result['target_probability'] > 0.5 else 'False'} answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7610a",
   "metadata": {},
   "source": [
    "## 4. Example 3: Comparing Explanations for Different Answers\n",
    "\n",
    "Let's explain why the model chose each possible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64224d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 3: Comparing Attributions Across Answer Choices\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use MedGemma for this example\n",
    "comparison_prompt = \"\"\"A patient presents with polyuria, polydipsia, and weight loss. \n",
    "Blood glucose is 350 mg/dL. What is the diagnosis?\n",
    "\n",
    "A) Type 1 diabetes\n",
    "B) Type 2 diabetes  \n",
    "C) Diabetes insipidus\n",
    "D) Hyperthyroidism\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Get prediction\n",
    "print(\"\\n[Model Prediction]\")\n",
    "response = medgemma.generate(comparison_prompt)\n",
    "print(f\"Answer: {medgemma.last_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain each answer choice\n",
    "ig = MedicalIntegratedGradients(medgemma, n_steps=50, verbose=False)\n",
    "\n",
    "print(\"\\n[Computing attributions for each answer choice...]\\n\")\n",
    "\n",
    "for choice in ['A', 'B', 'C', 'D']:\n",
    "    print(f\"\\nExplaining Answer {choice}:\")\n",
    "    result = ig.attribute(comparison_prompt, choice, return_convergence_delta=True)\n",
    "    \n",
    "    # Get top 3 tokens\n",
    "    token_scores = list(zip(result['tokens'], result['attributions']))\n",
    "    token_scores.sort(key=lambda x: x[1], reverse=True)  # Sort by attribution\n",
    "    \n",
    "    print(f\"  Probability: {result['target_probability']:.4f}\")\n",
    "    print(f\"  Top 3 supporting tokens:\")\n",
    "    for i, (token, score) in enumerate(token_scores[:3], 1):\n",
    "        clean_token = token.replace('▁', ' ').replace('Ġ', ' ').strip()\n",
    "        if clean_token:\n",
    "            print(f\"    {i}. '{clean_token}': {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad602f6",
   "metadata": {},
   "source": [
    "## 5. Example 4: Batch Processing Multiple Questions\n",
    "\n",
    "Explain multiple predictions efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 4: Batch Explanation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define multiple questions\n",
    "questions = [\n",
    "    {\n",
    "        'prompt': \"\"\"Aspirin works by irreversibly inhibiting cyclooxygenase enzymes.\n",
    "A) True\n",
    "B) False\n",
    "Answer:\"\"\",\n",
    "        'target': 'A'\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"\"\"Beta-blockers are contraindicated in acute asthma exacerbation.\n",
    "A) True  \n",
    "B) False\n",
    "Answer:\"\"\",\n",
    "        'target': 'A'\n",
    "    },\n",
    "    {\n",
    "        'prompt': \"\"\"ACE inhibitors can cause hyperkalemia.\n",
    "A) True\n",
    "B) False\n",
    "Answer:\"\"\",\n",
    "        'target': 'A'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Switch apollo to yn mode\n",
    "apollo.set_task(\"yn\")\n",
    "\n",
    "# Batch attribution\n",
    "ig_apollo = MedicalIntegratedGradients(apollo, n_steps=30, verbose=False)\n",
    "\n",
    "print(\"\\n[Processing batch...]\\n\")\n",
    "\n",
    "prompts = [q['prompt'] for q in questions]\n",
    "targets = [q['target'] for q in questions]\n",
    "\n",
    "results = ig_apollo.attribute_batch(prompts, targets)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n[Summary]\\n\")\n",
    "for i, (q, result) in enumerate(zip(questions, results), 1):\n",
    "    first_line = q['prompt'].split('\\n')[0][:60] + \"...\"\n",
    "    print(f\"{i}. {first_line}\")\n",
    "    print(f\"   Prediction: {result['prediction']} | Target Prob: {result['target_probability']:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5aec7",
   "metadata": {},
   "source": [
    "## 6. Custom Visualization\n",
    "\n",
    "Create your own visualizations of token attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE 5: Custom Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use a previous result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get attributions for a question\n",
    "custom_prompt = \"\"\"Patient has fever, productive cough, and consolidation on chest X-ray.\n",
    "A) Pneumonia\n",
    "B) Tuberculosis\n",
    "C) Lung cancer\n",
    "D) Heart failure\n",
    "Answer:\"\"\"\n",
    "\n",
    "medgemma.set_task(\"mcq\")\n",
    "ig_medgemma = MedicalIntegratedGradients(medgemma, n_steps=50, verbose=True)\n",
    "\n",
    "result = ig_medgemma.attribute(custom_prompt, \"A\")\n",
    "\n",
    "# Bar chart\n",
    "tokens = [t.replace('▁', '').replace('Ġ', '') for t in result['tokens']]\n",
    "attributions = result['attributions']\n",
    "\n",
    "# Show top 15 tokens\n",
    "top_indices = np.argsort(np.abs(attributions))[-15:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(top_indices)), attributions[top_indices])\n",
    "plt.yticks(range(len(top_indices)), [tokens[i] for i in top_indices])\n",
    "plt.xlabel('Attribution Score')\n",
    "plt.title(f'Top 15 Token Attributions for Answer {result[\"target_class\"]}')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrediction: {result['prediction']}\")\n",
    "print(f\"Target Probability: {result['target_probability']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b92021",
   "metadata": {},
   "source": [
    "## 7. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45775d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "\n",
    "del medgemma\n",
    "del apollo\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"✓ Memory cleaned up\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Medical Integrated Gradients Demo - COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04631e3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we learned:\n",
    "\n",
    "1. **Basic Usage**: Load wrapper → Create IG explainer → Get attributions\n",
    "2. **Interpretation**: Positive attributions support the answer, negative oppose it\n",
    "3. **Convergence**: Lower convergence delta = more accurate attributions\n",
    "4. **Comparison**: Explain why model chose one answer over others\n",
    "5. **Batch Processing**: Efficiently explain multiple predictions\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Clinical terms** (symptoms, test results) typically have high attributions\n",
    "- **Question structure words** (\"What is\", \"diagnosis\") have lower importance\n",
    "- **Answer options** themselves can have significant attributions\n",
    "- **Model differences**: MedGemma vs Apollo may focus on different tokens\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Compare IG with other XAI methods (TokenSHAP, LIME)\n",
    "- Analyze attribution patterns across medical domains\n",
    "- Use attributions to improve prompt engineering\n",
    "- Build trust in model predictions through explanations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
